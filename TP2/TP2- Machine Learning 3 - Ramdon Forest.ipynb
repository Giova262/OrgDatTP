{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "%matplotlib inline\n",
    "import math \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "import category_encoders as ce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos.\n",
    "\n",
    "train = pd.read_csv('./train.csv'\n",
    "            ,dtype={\n",
    "             \"id\":np.int32\n",
    "            ,\"titulo\":str\n",
    "            ,\"fecha\":str \n",
    "            ,\"ciudad\": str\n",
    "            ,\"provincia\": str\n",
    "            ,\"descripcion\": str  \n",
    "            ,\"direccion\": str  \n",
    "            ,\"tipodepropiedad\": 'category' })\n",
    "\n",
    "test = pd.read_csv('./test.csv'\n",
    "            ,dtype={\n",
    "             \"id\":np.int32\n",
    "            ,\"titulo\":str\n",
    "            ,\"fecha\":str \n",
    "            ,\"ciudad\": str\n",
    "            ,\"provincia\": str\n",
    "            ,\"descripcion\": str  \n",
    "            ,\"direccion\": str \n",
    "            ,\"tipodepropiedad\": 'category' })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrica RMSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de evaluación\n",
    "def RMSLE(actual, pred):\n",
    "    return (np.mean((np.log(actual + 1) - np.log(pred + 1)) ** 2)) **.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nulls( a ):\n",
    "\n",
    "    a[['metroscubiertos']] = a[['metroscubiertos']].fillna(value= (a['metroscubiertos'].mean())  )\n",
    "    a[['antiguedad']] = a[['antiguedad']].fillna(value= (a['antiguedad'].mean()) )\n",
    "    a[['habitaciones']] = a[['habitaciones']].fillna(value= (a['habitaciones'].mean()) )\n",
    "    a[['banos']] = a[['banos']].fillna(value= (a['habitaciones'].max()))\n",
    "    a[['idzona']] = a[['idzona']].fillna(value=(a['idzona'].median()))\n",
    "    a[['garages']] = a[['garages']].fillna(value=(a['garages'].min()))\n",
    "    a[['metrostotales']] = a[['metrostotales']].fillna(value=(a['metrostotales'].mean()))\n",
    "    a[['lng']] = a[['lng']].fillna(value=(a['lng'].median()))\n",
    "    a[['lat']] = a[['lat']].fillna(value=(a['lat'].median()))\n",
    "    a[['centroscomercialescercanos']] = a[['centroscomercialescercanos']].fillna(value=(a['metrostotales'].median()) or 0 )\n",
    "    a[['piscina']] = a[['piscina']].fillna(value= (a['piscina'].min()) )\n",
    "    a[['gimnasio']] = a[['gimnasio']].fillna(value= (a['gimnasio'].min()) )\n",
    "    a[['usosmultiples']] = a[['usosmultiples']].fillna(value= (a['usosmultiples'].min()) )\n",
    "    \n",
    "    a[['provincia']] = a[['provincia']].fillna(value= str(a['provincia'].mode()) )\n",
    "    a[['ciudad']] = a[['ciudad']].fillna(value= str(a['ciudad'].mode()) )\n",
    "    a[['descripcion']] = a[['descripcion']].fillna(value= str(a['descripcion'].mode()) )\n",
    "    a[['titulo']] = a[['titulo']].fillna(value= str(a['titulo'].mode()) )\n",
    "    a[['direccion']] = a[['direccion']].fillna(value= str(a['direccion'].mode()) )\n",
    "    a[['fecha']] = a[['fecha']].fillna(value= str( a['fecha'].mode() ) )\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularError(model,X_test,X_train ,y_test,y_train):\n",
    "    #Calculo de Errores contra mi set de test\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    #Calculo de Errores contra mi set de entrenamiento\n",
    "\n",
    "    rmsle_test = RMSLE(y_test, pred)\n",
    "    rmsle_train = RMSLE(y_train, model.predict(X_train) )\n",
    "\n",
    "    print(f\"RMSLE Error (train): {rmsle_train:.5f}\")\n",
    "    print(f\"RMSLE Error (Test): {rmsle_test:.5f}\")\n",
    "\n",
    "\n",
    "    mae_test = mean_absolute_error(y_test, pred)\n",
    "    mae_train = mean_absolute_error(y_train, model.predict(X_train) )\n",
    "\n",
    "    print(f\"MAE Error (train): {mae_train:.5f}\")\n",
    "    print(f\"MAE Error (Test): {mae_test:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generarCsv(model):\n",
    "    \n",
    "    c = test\n",
    "\n",
    "    c[['tipodepropiedad']] = c[['tipodepropiedad']].fillna(value= 'Casa')\n",
    "    c[['fecha']] = c[['fecha']].fillna(value= ( a['fecha'].mode() ) )\n",
    "\n",
    "    c = fill_nulls(c)\n",
    "\n",
    "    one_hot_enc = ce.OneHotEncoder()\n",
    "    one_hot_encoded = one_hot_enc.fit_transform(c['tipodepropiedad'])\n",
    "    c = c.join(one_hot_encoded.add_suffix(\"_oh\"))\n",
    "\n",
    "    c['fecha'] = pd.to_datetime(c['fecha'])\n",
    "    c['anio'] = c['fecha'].dt.year\n",
    "    c['mes'] = c['fecha'].dt.month\n",
    "    c['dia'] = c['fecha'].dt.dayofweek\n",
    "    \n",
    "    c['titulo'] = c['titulo'].apply(len)\n",
    "    c['descripcion'] = c['descripcion'].apply(len)\n",
    "    c['direccion'] = c['direccion'].apply(len)\n",
    "    c['ciudad'] = c['ciudad'].apply(len)\n",
    "    c['provincia'] = c['provincia'].apply(len)\n",
    "    \n",
    "    c['metrostotales'] = np.log(c['metrostotales'])\n",
    "    c['metroscubiertos'] = np.log(c['metroscubiertos'])\n",
    "\n",
    "\n",
    "    b = c[features]\n",
    "\n",
    "    #Exporto csv con la prediccion\n",
    "    pred = model.predict(b) \n",
    "    #Creo un dataframe con el formato ( id , precio (El precio predecido ))\n",
    "    res = test['id'].to_frame()\n",
    "    res.insert(1,'target', pred , True) \n",
    "    #Exportamos la prediccion en formato csv sin labels\n",
    "    res.to_csv(r'C:\\Users\\Giova\\JupiterLabFolder\\Archivos CSV\\res.csv',index=False , header=True)\n",
    "    print('CSV creado Exitosamente.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " features = [ 'anio',\n",
    "    \n",
    "              'mes',\n",
    "              'dia',\n",
    "              'titulo',\n",
    "              'descripcion',\n",
    "              'direccion',\n",
    "              'ciudad',\n",
    "              'provincia',\n",
    "              \n",
    "              'metrostotales',\n",
    "              'escuelascercanas',\n",
    "              'antiguedad',\n",
    "              'centroscomercialescercanos',\n",
    "              'garages',\n",
    "              'piscina',\n",
    "              'metroscubiertos',\n",
    "              'banos',\n",
    "              'habitaciones',\n",
    "              'lat','lng','idzona',\n",
    "              'gimnasio',\n",
    "              'usosmultiples',\n",
    "              'tipodepropiedad_1_oh',\n",
    "              'tipodepropiedad_2_oh',\n",
    "              'tipodepropiedad_3_oh',\n",
    "              'tipodepropiedad_4_oh',\n",
    "              'tipodepropiedad_5_oh',\n",
    "              'tipodepropiedad_6_oh',\n",
    "              'tipodepropiedad_7_oh',\n",
    "              'tipodepropiedad_8_oh',\n",
    "              'tipodepropiedad_9_oh',\n",
    "              'tipodepropiedad_10_oh',\n",
    "              'tipodepropiedad_11_oh',\n",
    "              'tipodepropiedad_12_oh',\n",
    "              'tipodepropiedad_13_oh',\n",
    "              'tipodepropiedad_14_oh',\n",
    "              'tipodepropiedad_15_oh',\n",
    "              'tipodepropiedad_16_oh',\n",
    "              'tipodepropiedad_17_oh',\n",
    "              'tipodepropiedad_18_oh',\n",
    "              'tipodepropiedad_19_oh',\n",
    "              'tipodepropiedad_20_oh',\n",
    "              'tipodepropiedad_21_oh',\n",
    "              'tipodepropiedad_22_oh' ]\n",
    "    \n",
    "len( features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train\n",
    "\n",
    "a = a.loc[ ( a['tipodepropiedad'] != 'Hospedaje'  ) & ( a['tipodepropiedad'] != 'Garage'  )  ]\n",
    "\n",
    "a[['tipodepropiedad']] = a[['tipodepropiedad']].fillna(value='Casa')\n",
    "\n",
    "nulls = pd.DataFrame( a.isna().sum().sort_values(ascending=False) , columns=['Nulls'] )\n",
    "nulls['%porcentaje'] = round(100*nulls['Nulls'] / len(a), 2)\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fill_nulls(a)\n",
    "\n",
    "print('Nulls cambiados !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame( a.isna().sum().sort_values(ascending=False) , columns=['Nulls'] )\n",
    "nulls['%porcentaje'] = round(100*nulls['Nulls'] / len(a), 2)\n",
    "nulls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding variables categoricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc = ce.OneHotEncoder()\n",
    "one_hot_encoded = one_hot_enc.fit_transform(a['tipodepropiedad'])\n",
    "a= a.join(one_hot_encoded.add_suffix(\"_oh\"))\n",
    "print('One Hot encoding done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacion de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['fecha'] = pd.to_datetime(a['fecha'])\n",
    "\n",
    "a['anio'] = a['fecha'].dt.year\n",
    "a['mes'] = a['fecha'].dt.month\n",
    "a['dia'] = a['fecha'].dt.dayofweek\n",
    "\n",
    "a['titulo'] = a['titulo'].apply(len)\n",
    "a['descripcion'] = a['descripcion'].apply(len)\n",
    "a['direccion'] = a['direccion'].apply(len)\n",
    "a['ciudad'] = a['ciudad'].apply(len)\n",
    "a['provincia'] = a['provincia'].apply(len)\n",
    "\n",
    "a['metrostotales'] = np.log(a['metrostotales'])\n",
    "a['metroscubiertos'] = np.log(a['metroscubiertos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vistazo de como queda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"\\nTamaño : {a.shape}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Prediccion sin dividir el set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split del set de datos General ( no sub dividi por años )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = a[features]\n",
    "\n",
    "y = a['precio']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor( random_state=0,bootstrap =False,min_samples_leaf=2 ,n_jobs=-1,\n",
    "                             max_depth = 50,max_features = 21 ,min_samples_split = 2 ,\n",
    "                             n_estimators = 175 )\n",
    "\n",
    "#Entreno\n",
    "\n",
    "model = regr.fit(X_train, y_train)\n",
    "\n",
    "print('Finalizo el entrenamiento\\n\\n')\n",
    "\n",
    "calcularError(model,X_test,X_train ,y_test,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest  + Random Grid + Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 400, num = 15)]\n",
    "max_features = [int(x) for x in np.linspace(start = 4, stop = 44, num = 15)]\n",
    "max_depth = [int(x) for x in np.linspace(50, 100, num = 10)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [ False]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "model = rf_random.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Muestra los mejores paramentros encontrados\n",
    "print( rf_random.best_params_ )\n",
    "print('Finalizo el random search + Cross validation \\n\\n')\n",
    "calcularError(model,X_test,X_train ,y_test,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generarCsv(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VottingRegresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modelos\n",
    "\n",
    "r1 = RandomForestRegressor(random_state=0,bootstrap =False,min_samples_leaf=2 ,n_jobs=-1,\n",
    "                           max_depth = 50,max_features = 21 ,min_samples_split = 2 ,\n",
    "                           n_estimators = 175 )\n",
    "\n",
    "r2 = KNeighborsRegressor(50)\n",
    "\n",
    "r3 = DummyRegressor(strategy='mean')\n",
    "\n",
    "r4 = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "er = VotingRegressor([('rf', r1),('rf2', r3),('rf3', r4),('rf4', r2)],weights =[1,0.01,0.1,0.75])\n",
    "     \n",
    "    \n",
    "# Train model\n",
    "model = er.fit(X_train, y_train)\n",
    "\n",
    "print('Termino de entrenar el modelo\\n\\n')\n",
    "calcularError(model,X_test,X_train ,y_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generacion de csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generarCsv(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Predicciones por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copio el train para trabajar con él\n",
    "a = train\n",
    "\n",
    "a = a.loc[ ( a['tipodepropiedad'] != 'Hospedaje'  ) & ( a['tipodepropiedad'] != 'Garage'  )  ]\n",
    "a[['tipodepropiedad']] = a[['tipodepropiedad']].fillna(value='Casa')\n",
    "\n",
    "a = fill_nulls(a)\n",
    "\n",
    "print('filtrado y nulls hecho!\\n')\n",
    "\n",
    "nulls = pd.DataFrame( a.isna().sum().sort_values(ascending=False) , columns=['Nulls'] )\n",
    "nulls['%porcentaje'] = round(100*nulls['Nulls'] / len(a), 2)\n",
    "nulls.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "\n",
    "one_hot_enc = ce.OneHotEncoder()\n",
    "one_hot_encoded = one_hot_enc.fit_transform(a['tipodepropiedad'])\n",
    "a= a.join(one_hot_encoded.add_suffix(\"_oh\"))\n",
    "print('One Hot encoding done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformacion de variables\n",
    "\n",
    "a['fecha'] = pd.to_datetime(a['fecha'])\n",
    "\n",
    "a['anio'] = a['fecha'].dt.year\n",
    "a['mes'] = a['fecha'].dt.month\n",
    "a['dia'] = a['fecha'].dt.dayofweek\n",
    "\n",
    "\n",
    "a['titulo'] = a['titulo'].apply(len)\n",
    "a['descripcion'] = a['descripcion'].apply(len)\n",
    "a['direccion'] = a['direccion'].apply(len)\n",
    "a['ciudad'] = a['ciudad'].apply(len)\n",
    "a['provincia'] = a['provincia'].apply(len)\n",
    "\n",
    "a['metrostotales'] = np.log(a['metrostotales'])\n",
    "a['metroscubiertos'] = np.log(a['metroscubiertos'])\n",
    "\n",
    "print('Varibales transformadas !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividi por año\n",
    "\n",
    "a2012 = a.loc[ a['anio'] == 2012 ]\n",
    "a2013 = a.loc[ a['anio'] == 2013 ]\n",
    "a2014 = a.loc[ a['anio'] == 2014 ]\n",
    "a2015 = a.loc[ a['anio'] == 2015 ]\n",
    "a2016 = a.loc[ a['anio'] == 2016 ]\n",
    "\n",
    "print('Particionado !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para el año 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el set de datos 2012 lleno los nulls\n",
    "\n",
    "a2012 = fill_nulls(a2012)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "X = a2012[features]\n",
    "\n",
    "y = a2012['precio']\n",
    "\n",
    "# Pariciono los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "print('Separacion del set de datos exitosamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search + Cross Validation\n",
    "\n",
    "# {'n_estimators': 244, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 21, 'max_depth': 88, 'bootstrap': False}\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 400, num = 10)]\n",
    "max_features = [int(x) for x in np.linspace(start = 4, stop = 36, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(50, 100, num = 10)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "#Modelo random forest\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 2, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "\n",
    "model2012 = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "print('Modelo hecho!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultados\n",
    "calcularError(model2012,X_test,X_train ,y_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el set de datos 2013 lleno los nulls\n",
    "\n",
    "a2013 = fill_nulls(a2013)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "X = a2013[features]\n",
    "\n",
    "y = a2013['precio']\n",
    "\n",
    "# Pariciono los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "print('Separacion del set de datos exitosamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search + Cross Validation\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 400, num = 10)]\n",
    "max_features = [int(x) for x in np.linspace(start = 4, stop = 36, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(50, 100, num = 10)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 2, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "\n",
    "model2013 = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "print('Modelo hecho!')\n",
    "\n",
    "#{'n_estimators': 244, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 21, 'max_depth': 88, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultados\n",
    "\n",
    "calcularError(model2013,X_test,X_train ,y_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el set de datos 2012 lleno los nulls\n",
    "\n",
    "a2014 = fill_nulls(a2014)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "X = a2014[features]\n",
    "\n",
    "y = a2014['precio']\n",
    "\n",
    "# Pariciono los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Separacion del set de datos exitosamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search + Cross Validation\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 400, num = 20)]\n",
    "max_features = [int(x) for x in np.linspace(start = 4, stop = 36, num = 15)]\n",
    "max_depth = [int(x) for x in np.linspace(50, 100, num = 20)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 2, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "\n",
    "model2014 = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "print('Modelo hecho!')\n",
    "\n",
    "#{'n_estimators': 381, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 17, 'max_depth': 81, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultados\n",
    "\n",
    "calcularError(model2014,X_test,X_train ,y_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el set de datos 2012 lleno los nulls\n",
    "\n",
    "a2015 = fill_nulls(a2015)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "X = a2015[features]\n",
    "\n",
    "y = a2015['precio']\n",
    "\n",
    "# Pariciono los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Separacion del set de datos exitosamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search + Cross Validation\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 400, num = 20)]\n",
    "max_features = [int(x) for x in np.linspace(start = 4, stop = 36, num = 15)]\n",
    "max_depth = [int(x) for x in np.linspace(50, 100, num = 20)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 2, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "\n",
    "model2015 = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "print('Modelo hecho!')\n",
    "\n",
    "#{'n_estimators': 381, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 17, 'max_depth': 81, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultados\n",
    "\n",
    "calcularError(model2015,X_test,X_train ,y_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el set de datos 2012 lleno los nulls\n",
    "a2016 = pd.concat([a2012,a2013,a2014,a2015,a2016], ignore_index=False)\n",
    "a2016 = a2016.sort_index()\n",
    "a2016 = fill_nulls(a2016)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "X = a2016[features]\n",
    "\n",
    "y = a2016['precio']\n",
    "\n",
    "# Pariciono los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Separacion del set de datos exitosamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search + Cross Validation\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 400, num = 10)]\n",
    "max_features = [int(x) for x in np.linspace(start = 4, stop = 44, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(50, 100, num = 20)]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [ False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 2, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, random_state=42, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "\n",
    "model2016 = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "print('Modelo hecho!')\n",
    "\n",
    "#{'n_estimators': 381, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 16, 'max_depth': 60, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultados\n",
    "calcularError(model2016,X_test,X_train ,y_test,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones del Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo el set de test\n",
    "\n",
    "c = test\n",
    "\n",
    "c = c.loc[ ( c['tipodepropiedad'] != 'Hospedaje'  ) & ( c['tipodepropiedad'] != 'Garage'  )  ]\n",
    "c[['tipodepropiedad']] = c[['tipodepropiedad']].fillna(value='Casa')\n",
    "\n",
    "c = fill_nulls(c)\n",
    "\n",
    "# One hot encoding\n",
    "\n",
    "one_hot_enc = ce.OneHotEncoder()\n",
    "one_hot_encoded = one_hot_enc.fit_transform(c['tipodepropiedad'])\n",
    "c= c.join(one_hot_encoded.add_suffix(\"_oh\"))\n",
    "print('One Hot encoding done!')\n",
    "\n",
    "#Transformacion de variables\n",
    "\n",
    "c['fecha'] = pd.to_datetime(c['fecha'])\n",
    "\n",
    "c['anio'] = c['fecha'].dt.year\n",
    "c['mes'] = c['fecha'].dt.month\n",
    "c['dia'] = c['fecha'].dt.dayofweek\n",
    "\n",
    "c['titulo'] = c['titulo'].apply(len)\n",
    "c['descripcion'] = c['descripcion'].apply(len)\n",
    "c['direccion'] = c['direccion'].apply(len)\n",
    "c['ciudad'] = c['ciudad'].apply(len)\n",
    "c['provincia'] = c['provincia'].apply(len)\n",
    "\n",
    "c['metrostotales'] = np.log(c['metrostotales'])\n",
    "c['metroscubiertos'] = np.log(c['metroscubiertos'])\n",
    "\n",
    "\n",
    "print('Varibales transformadas !')\n",
    "\n",
    "# Dividi por año\n",
    "\n",
    "c2012 = c.loc[ c['anio'] == 2012 ]\n",
    "c2013 = c.loc[ c['anio'] == 2013 ]\n",
    "c2014 = c.loc[ c['anio'] == 2014 ]\n",
    "c2015 = c.loc[ c['anio'] == 2015 ]\n",
    "c2016 = c.loc[ c['anio'] == 2016 ]\n",
    "\n",
    "print('Particionado !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el año 2016 \n",
    "\n",
    "c2016 = fill_nulls(c2016)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "d = c2016[features]\n",
    "\n",
    "#Exporto csv con la prediccion\n",
    "pred2016 = model2016.predict(d)\n",
    "\n",
    "res2016 = c2016['id'].to_frame()\n",
    "res2016.insert(1,'target', pred2016 , True) \n",
    "\n",
    "#res2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015\n",
    "\n",
    "c2015 = fill_nulls(c2015)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "d = c2015[features]\n",
    "\n",
    "#Exporto csv con la prediccion\n",
    "pred2015 = model2015.predict(d)\n",
    "\n",
    "res2015 = c2015['id'].to_frame()\n",
    "res2015.insert(1,'target', pred2015 , True) \n",
    "\n",
    "#res2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014\n",
    "\n",
    "c2014 = fill_nulls(c2014)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "d = c2014[features]\n",
    "\n",
    "#Exporto csv con la prediccion\n",
    "pred2014 = model2014.predict(d)\n",
    "\n",
    "res2014 = c2014['id'].to_frame()\n",
    "res2014.insert(1,'target', pred2014 , True) \n",
    "\n",
    "#res2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2013\n",
    "\n",
    "c2013 = fill_nulls(c2013)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "d = c2013[features]\n",
    "\n",
    "#Exporto csv con la prediccion\n",
    "pred2013 = model2013.predict(d)\n",
    "\n",
    "res2013 = c2013['id'].to_frame()\n",
    "res2013.insert(1,'target', pred2013 , True) \n",
    "\n",
    "#res2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012\n",
    "\n",
    "c2012 = fill_nulls(c2012)\n",
    "\n",
    "# Creo los set de test y train\n",
    "\n",
    "d = c2012[features]\n",
    "\n",
    "#Exporto csv con la prediccion\n",
    "pred2012 = model2012.predict(d)\n",
    "\n",
    "res2012 = c2012['id'].to_frame()\n",
    "res2012.insert(1,'target', pred2012 , True) \n",
    "\n",
    "#res2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armo el csv final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resfinal = pd.concat([res2012,res2013,res2014,res2015,res2016], ignore_index=False)\n",
    "resfinal = resfinal.sort_index()\n",
    "\n",
    "#Exportamos la prediccion en formato csv sin labels\n",
    "resfinal.to_csv(r'C:\\Users\\Giova\\JupiterLabFolder\\Archivos CSV\\res.csv',index=False , header=True)\n",
    "print('CSV creado Exitosamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
